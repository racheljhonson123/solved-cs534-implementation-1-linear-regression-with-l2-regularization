Download Link: https://assignmentchef.com/product/solved-cs534-implementation-1-linear-regression-with-l2-regularization
<br>
For the first part of the assignment, you need to implement linear regression with <em>L</em><sub>2 </sub>(quadratic) regularization, which learns from a set of <em>N </em>training examples  an weight vector <strong>w </strong>that optimize the following regularized Sum of Squared Error (SSE) objective:

<em>N</em>

X(<em>y </em>−<strong>w</strong><em>T</em><strong>x </strong>)2 + <em>λ</em>k<strong>w</strong>k2                 (1) <em><sub>i             </sub></em><em><sub>i</sub></em>

<em>i</em>=1

To optimize this objective, you need to implement the gradient descent algorithm. Because some features have very large values, for part of the assignment you are asked to normalize the features to the range between zero and one. This will have an impact on the convergence behavior of gradient descent.

<strong>Data. </strong>The dataset consisted of historic data on houses sold between May 2014 to May 2015. You need to build a linear regression that can be used to predict the house’s price based on a set of features. You are provided with three data files: <strong>train</strong>, <strong>test </strong>and <strong>validation</strong>, all in csv format. You are provided with a description of the features as well. The first column of each file contains the dummy feature taking the constant value of 1 for all examples. The last column in the files <strong>train </strong>and <strong>validation </strong>stores the target <em>y </em>values for each example, We omitted <em>y </em>values from <strong>test </strong>file. You need to learn from the training data and tune your parameters with the provided validation data to chose the best model. Your submission will include a prediction file of the testing data that has the predicted <em>y </em>values generated by the best model you learned.

<strong>General guidelines for training. </strong>For all parts, you should train your model until the convergence condition is met, i.e., the norm of the gradient is less than <em> </em>= 0.5. If you find that this specific threshold makes the training time too long for some learning rate values, feel free to use higher values and report the value you used. It is a good practice to monitor the norm of the gradient during the training. You need to report the SSE (the first term in the Eq. 1 ) on the training data and the validation data respectively for each value of the hyperparamter you tune (e.g. learning rate, <em>λ</em>). Use the best model you learned to do predction on the test data and submit the prediction file.

<strong>Part 0 (10 pts) : Preprocessing and simple analysis. </strong>Perform the following preprocessing of the your data.

<ul>

 <li>Remove the ID feature. Why do you think it is a bad idea to use this feature in learning?</li>

 <li>Split the date feature into three separate numerical features: <em>month</em>, <em>day </em>, and <em>year</em>. Can you think of better ways of using this date feature?</li>

 <li>Build a table that reports the statistics for each feature. For numerical features, please report the mean, the standard deviation, and the range. For categorical features such as waterfront, grade, condition (the later two are ordinal), please report the percentage of examples for each category.</li>

 <li>Based on the meaning of the features as well as the statistics, which set of features do you expect to be useful for this task? Why?</li>

 <li>Normalize all features to the range between 0 and 1 using the training data. Note that when you apply the learned model from the normalized data to test data, you should make sure that you are using the same normalizing procedure as used in training.</li>

</ul>

<strong>Part 1 (30 pts). Explore different learning rate for batch gradient descent. </strong>For this part, you will work with the preprocessed and normalized data and fix <em>λ </em>to 0 and consider at least the following values for the learning rate: 10<sup>0</sup><em>,</em>10<sup>−1</sup><em>,</em>10<sup>−2</sup><em>,</em>10<sup>−3</sup><em>,</em>10<sup>−4</sup><em>,</em>10<sup>−5</sup><em>,</em>10<sup>−6</sup><em>,</em>10<sup>−7</sup>.

<ul>

 <li>Which learning rate or learning rates did you observe to be good for this particular dataset? What learning rates make the gradient decent explode? Report your observations together with some example curves showing the training SSE as a function of training iterations and its convergence or non-convergence behaviors.</li>

 <li>For each learning rate worked for you, Report the SSE on the training data and the validation data respectively and the number of iterations needed to achieve the convergence condition for training. What do you observe?</li>

 <li>Use the validation data to pick the best converged solution, and report the learned weights for each feature. Which feature are the most important in deciding the house prices according to the learned weights? Compare them to your pre-analysis results (Part 0 (d)).</li>

</ul>

<strong>Part2 (30 pts). Experiments with different </strong><em>λ </em><strong>values. </strong>For this part, you will test the effect of the regularization parameter on your linear regressor. Please exclude the bias term from regularization. It is often the case that we don’t really what the right <em>λ </em>value should be and we will need to consider a range of different <em>λ </em>values. For this project, consider at least the following values for <em>λ</em>: 0<em>,</em>10<sup>−3</sup><em>,</em>10<sup>−2</sup><em>,</em>10<sup>−1</sup><em>,</em>1<em>,</em>10<em>,</em>100. Feel free to explore other choices of <em>λ </em>using a broader or finer search grid. Report the SSE on the training data and the validation data respectively for each value of <em>λ</em>. Report the weights you learned for different values of <em>λ</em>. What do you observe? Your discussion of the results should clearly answer the following questions:

<ul>

 <li>What trend do you observe from the training SSE as we change <em>λ </em>value?</li>

 <li>What tread do you observe from the validation SSE?</li>

 <li>Provide an explanation for the observed behaviors.</li>

 <li>What features get turned off for <em>λ</em>= 10, 10<sup>−2 </sup>and 0 ?</li>

</ul>

<strong>Part 3 (10 pts). Training with non-normalized data </strong>Use the preprocessed data but skip the normalization. Consider at least the following values for learning rate: 1, 0<em>,</em>10<sup>−3</sup><em>,</em>10<sup>−6</sup><em>,</em>10<sup>−9</sup><em>,</em>10<sup>−15</sup>. For each value , train up to 10000 iterations ( Fix the number of iterations for this part). If training is clearly diverging, you can terminate early. Plot the training SSE and validation SSE respectively as a function of the number of iterations. What do you observe? Specify the learning rate value (if any) that prevents the gradient descent from exploding? Compare between using the normalized and the non-normalized versions of the data. Which one is easier to train and why?